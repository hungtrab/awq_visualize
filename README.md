# AWQ + Marlin Kernel Demo

ğŸš€ **Comprehensive demonstrations** cá»§a cÃ¡ch AWQ quantization vÃ  Marlin kernel hoáº¡t Ä‘á»™ng cÃ¹ng nhau Ä‘á»ƒ tÄƒng tá»‘c LLM inference.

## ğŸ“‹ Tá»•ng Quan

Project nÃ y cung cáº¥p:
- âœ… **AWQ Quantization Demo**: Hiá»ƒu cÃ¡ch AWQ quantize weights tá»« FP16 â†’ INT4
- âœ… **Marlin Kernel Demo**: Xem cÃ¡ch Marlin kernel thá»±c hiá»‡n FP16Ã—INT4 matmul
- âœ… **GPU Introspection**: "X-ray view" cá»§a GPU state trong khi cháº¡y kernel
- âœ… **Performance Profiling**: So sÃ¡nh hiá»‡u nÄƒng across batch sizes
- âœ… **Visualization Tools**: Charts, plots, vÃ  diagrams

## ğŸ¯ Quick Start

### 1. Setup

```bash
# Clone repositories (Marlin, AutoAWQ)
./setup_repos.sh

# Install dependencies
pip install -r requirements.txt

# (Optional) Build Marlin kernel
cd external/marlin
python setup.py install
cd ../..
```

### 2. Run Demos

```bash
# Demo 1: AWQ Quantization
python demos/01_awq_quantization_demo.py --model facebook/opt-125m

# Demo 2: Marlin Kernel Basics  
python demos/02_marlin_kernel_basic.py --visualize-packing --verify

# Demo 3: GPU Introspection
python demos/05_gpu_introspection.py --snapshot --visualize --benchmark

# Demo 4: Performance Profiling
python demos/04_performance_profiler.py --batch-sizes 1 2 4 8 16 32
```

## ğŸ“š Demos Chi Tiáº¿t

### 01 - AWQ Quantization Demo

Demonstrates AWQ quantization process vá»›i visualizations:

```bash
python demos/01_awq_quantization_demo.py \
    --model facebook/opt-125m \
    --quick-test
```

**Output:**
- Weight distribution plots (before/after quantization)
- Quantization error analysis
- Memory reduction metrics
- Inference test

**Há»c Ä‘Æ°á»£c gÃ¬:**
- AWQ báº£o vá»‡ important weights vá»›i scaling factors
- 4Ã— memory reduction vá»›i <1% accuracy loss
- Activation-aware quantization vs naive quantization

---

### 02 - Marlin Kernel Basics

Xem cÃ¡ch Marlin kernel pack/unpack INT4 weights:

```bash
python demos/02_marlin_kernel_basic.py \
    --m 128 --n 512 --k 512 \
    --visualize-packing \
    --verify
```

**Output:**
- INT4 packing visualization
- FP16Ã—FP16 vs FP16Ã—INT4 comparison
- Memory layout diagrams
- Performance benchmarks

**Há»c Ä‘Æ°á»£c gÃ¬:**
- INT4 weight packing format
- On-the-fly dequantization
- Memory bandwidth savings

---

### 05 - GPU Introspection

Deep dive vÃ o GPU state:

```bash
# Quick snapshot
python demos/05_gpu_introspection.py --snapshot --visualize

# With benchmarking
python demos/05_gpu_introspection.py --benchmark

# Real-time monitoring (10 seconds)
python demos/05_gpu_introspection.py --monitor 10

# Show kernel config
python demos/05_gpu_introspection.py --kernel-config "128,4,1;256,1,1"
```

**Output:**
- GPU info (compute capability, memory, SMs)
- Memory hierarchy visualization
- Kernel launch configuration
- Real-time GPU metrics (utilization, temperature, power)
- Matrix multiplication benchmarks

**Há»c Ä‘Æ°á»£c gÃ¬:**
- GPU memory hierarchy (Global â†’ L2 â†’ L1/Shared â†’ Registers)
- Block/thread organization
- Memory bandwidth utilization
- SM occupancy

---

### 04 - Performance Profiler

Comprehensive performance analysis:

```bash
python demos/04_performance_profiler.py \
    --model-dim 4096 \
    --batch-sizes 1 2 4 8 16 32 64 \
    --output-dir ./results
```

**Output:**
- `performance_results.csv`: All benchmark data
- `throughput_scaling.png`: Throughput vs batch size
- `latency_comparison.png`: FP16 vs INT4 latency
- Terminal summary table

**Metrics:**
- Latency (ms)
- Throughput (tokens/s)
- TFLOPS
- Speedup
- Memory saved

---

## ğŸ–¼ï¸ Visualizations

Example outputs Ä‘Æ°á»£c táº¡o:

| File | Description |
|------|-------------|
| `awq_quantization_*.png` | Weight distributions, errors |
| `throughput_scaling.png` | Performance vs batch size |
| `latency_comparison.png` | FP16 vs INT4 comparison |
| `performance_results.csv` | Raw benchmark data |

## ğŸ”§ Utilities

### Visualization (`utils/visualization.py`)

```python
from utils.visualization import VisualizationHelper

viz = VisualizationHelper(output_dir="./results")

# Plot weight distributions
viz.plot_weight_distribution({
    'original': weights_fp16,
    'quantized': weights_int4
}, filename='weights.png')

# Compare performance
viz.plot_performance_comparison(results, metric='latency_ms')
```

### Profiling (`utils/profiling.py`)

```python
from utils.profiling import CUDATimer, Benchmarker, profile_scope

# Quick timing
with profile_scope("my_operation"):
    result = my_function()

# Detailed benchmarking
benchmarker = Benchmarker(num_iterations=100)
result = benchmarker.benchmark_function(my_function, "test")
benchmarker.print_results()
```

## ğŸ“Š Expected Performance

TrÃªn NVIDIA A100 GPU:

| Method | Latency | Throughput | Memory | Speedup |
|--------|---------|------------|--------|---------|
| FP16 Baseline | 10.5 ms | 95 tok/s | 16 GB | 1.0Ã— |
| AWQ + Marlin | 3.7 ms | 270 tok/s | 4 GB | 2.8Ã— |

**Note:** Actual Marlin kernel cÃ³ thá»ƒ faster hÆ¡n simulation trong demo.

## ğŸ“ Educational Materials

### Example GPU State Visualization

See [`GPU_INTROSPECTION_EXAMPLE.md`](GPU_INTROSPECTION_EXAMPLE.md) Ä‘á»ƒ xem detailed examples cá»§a:
- Kernel configuration visualization
- Memory hierarchy snapshots
- Thread block layouts
- Real-time monitoring displays

### Key Concepts

**AWQ Quantization:**
- Activation-aware scaling
- Outlier preservation
- Per-channel quantization
- Minimal accuracy loss

**Marlin Kernel:**
- FP16Ã—INT4 mixed-precision
- Asynchronous memory operations
- Pipelining & double buffering
- Shared memory optimization
- Tensor Core utilization

## ğŸ’» Requirements

### Hardware
- NVIDIA GPU vá»›i CUDA support
- Compute Capability â‰¥ 7.5 (recommended cho Marlin)
- Minimum 8GB VRAM

### Software
- Python 3.8+
- PyTorch 2.0+ vá»›i CUDA
- CUDA Toolkit 11.8+ (cho Marlin kernel compilation)

### Python Packages
See [`requirements.txt`](requirements.txt)

## ğŸ› Troubleshooting

**CUDA not available:**
```bash
# Check CUDA installation
nvcc --version
nvidia-smi

# Install CUDA-enabled PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Import errors:**
```bash
# Ensure utils are in path
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

**Marlin kernel compilation issues:**
```bash
# Check compute capability
python -c "import torch; print(torch.cuda.get_device_properties(0).major, torch.cuda.get_device_properties(0).minor)"

# May need newer nvcc
# Follow instructions at: https://github.com/IST-DASLab/marlin
```

## ğŸ“– Further Reading

- **AWQ Paper**: [AWQ: Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)
- **Marlin Paper**: [Marlin: Fast INT4 Inference](https://arxiv.org/abs/2408.11743)
- **Marlin GitHub**: https://github.com/IST-DASLab/marlin
- **AutoAWQ GitHub**: https://github.com/casper-hansen/AutoAWQ

## ğŸ¤ Contributing

Suggestions vÃ  improvements welcome! This is an educational demo project.

## ğŸ“„ License

This demo project is for educational purposes. Please check licenses of:
- Marlin kernel: IST-DASLab
- AutoAWQ: casper-hansen
- Individual model licenses from HuggingFace

## ğŸ™ Acknowledgments

- **IST-DASLab** for Marlin kernel
- **MIT Han Lab** for AWQ algorithm
- **casper-hansen** for AutoAWQ implementation
- **HuggingFace** for model hosting

---

**Author**: Demo created for understanding AWQ + Marlin internals  
**Last Updated**: January 2026

Happy learning! ğŸ‰
