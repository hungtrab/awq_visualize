"""
MARLIN KERNEL - Step-by-Step Annotated Execution
================================================

File n√†y gi·∫£i th√≠ch CHI TI·∫æT t·ª´ng b∆∞·ªõc Marlin kernel th·ª±c hi·ªán FP16√óINT4 matmul.
M·ªói phase ƒë∆∞·ª£c document r√µ r√†ng v·ªõi memory operations v√† thread organization.

Kernel th·ª±c hi·ªán: C[M, N] = A[M, K] @ B[N, K]^T
- A: FP16 activations
- B: INT4 weights (packed)
- C: FP16 output
"""

import torch
import numpy as np
from typing import Tuple

# ============================================================================
# PHASE 0: KERNEL LAUNCH CONFIGURATION
# ============================================================================

class MarlinKernelConfig:
    """
    Configuration cho Marlin kernel launch
    
    Marlin chia computation th√†nh grid of thread blocks:
    - Grid: 2D grid of blocks (gridDim.x, gridDim.y)
    - Block: 256 threads organized as (256, 1, 1)
    - Each block processes a tile of output matrix
    """
    
    def __init__(self, M: int, N: int, K: int):
        self.M = M  # Batch size / rows in A
        self.N = N  # Output dim / rows in B
        self.K = K  # Hidden dim
        
        # Tile sizes (tuned for performance)
        self.TILE_M = 16    # Each block processes 16 rows of A
        self.TILE_N = 64    # Each block processes 64 rows of B (cols of output)
        self.TILE_K = 128   # Process 128 elements of K per iteration
        
        # Grid configuration
        self.grid_x = (N + self.TILE_N - 1) // self.TILE_N  # S·ªë blocks theo N
        self.grid_y = (M + self.TILE_M - 1) // self.TILE_M  # S·ªë blocks theo M
        
        # Thread configuration
        self.threads_per_block = 256
        self.warps_per_block = self.threads_per_block // 32  # 8 warps
        
        # Memory configuration
        self.shared_mem_size = 48 * 1024  # 48 KB shared memory
        
    def print_config(self):
        print("=" * 80)
        print("MARLIN KERNEL LAUNCH CONFIGURATION")
        print("=" * 80)
        print(f"\nüìê Matrix Dimensions:")
        print(f"   A: [{self.M}, {self.K}] (FP16)")
        print(f"   B: [{self.N}, {self.K}] (INT4 packed)")
        print(f"   C: [{self.M}, {self.N}] (FP16)\n")
        
        print(f"üß± Tile Sizes:")
        print(f"   TILE_M = {self.TILE_M}")
        print(f"   TILE_N = {self.TILE_N}")
        print(f"   TILE_K = {self.TILE_K}\n")
        
        print(f"üéØ Grid Configuration:")
        print(f"   gridDim = ({self.grid_x}, {self.grid_y}, 1)")
        print(f"   Total blocks = {self.grid_x * self.grid_y}\n")
        
        print(f"üßµ Thread Configuration:")
        print(f"   threadDim = ({self.threads_per_block}, 1, 1)")
        print(f"   Warps per block = {self.warps_per_block}\n")
        
        print(f"üíæ Memory:")
        print(f"   Shared memory = {self.shared_mem_size / 1024:.0f} KB per block")
        print("=" * 80 + "\n")


# ============================================================================
# PHASE 1: MEMORY LOADING - Global Memory ‚Üí Shared Memory
# ============================================================================

def phase1_load_to_shared_memory(
    block_id_x: int,
    block_id_y: int,
    thread_id: int,
    A_global: torch.Tensor,
    B_packed_global: torch.Tensor,
    config: MarlinKernelConfig
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU T·ª™ GLOBAL MEMORY ‚Üí SHARED MEMORY
    
    M·ªói thread block load m·ªôt tile c·ªßa A v√† B v√†o shared memory.
    S·ª≠ d·ª•ng ASYNCHRONOUS COPY ƒë·ªÉ overlap v·ªõi computation.
    
    Thread Organization:
    - 256 threads cooperate ƒë·ªÉ load data
    - Each thread loads multiple elements (coalesced access)
    - Use __pipeline_memcpy_async() for async copy
    
    Args:
        block_id_x, block_id_y: Block indices trong grid
        thread_id: Thread index trong block (0-255)
        A_global: Full activation matrix [M, K] trong global memory
        B_packed_global: INT4 weights [N, K//2] trong global memory
        config: Kernel configuration
    
    Returns:
        A_shared, B_shared: Data loaded v√†o shared memory
    """
    
    print(f"\n{'='*80}")
    print(f"PHASE 1: LOADING TO SHARED MEMORY")
    print(f"Block [{block_id_x}, {block_id_y}] | Thread {thread_id}")
    print(f"{'='*80}\n")
    
    # Calculate tile boundaries
    m_start = block_id_y * config.TILE_M
    m_end = min(m_start + config.TILE_M, config.M)
    n_start = block_id_x * config.TILE_N
    n_end = min(n_start + config.TILE_N, config.N)
    
    print(f"üìç Tile boundaries:")
    print(f"   M: [{m_start}, {m_end})")
    print(f"   N: [{n_start}, {n_end})\n")
    
    # Allocate shared memory (simulated)
    A_shared = torch.zeros(config.TILE_M, config.TILE_K, dtype=torch.float16, device='cpu')
    B_shared = torch.zeros(config.TILE_N, config.TILE_K // 2, dtype=torch.uint8, device='cpu')
    
    # -------------------------------------------------------------------------
    # STEP 1.1: Load A tile (COALESCED ACCESS)
    # -------------------------------------------------------------------------
    # Each thread loads multiple consecutive elements
    # Thread i loads: A[m_start + (i % TILE_M), k_start + (i // TILE_M)]
    
    print(f"üîÑ Step 1.1: Loading A tile (FP16)")
    print(f"   Source: A_global[{m_start}:{m_end}, 0:{config.TILE_K}]")
    print(f"   Dest:   A_shared[0:{config.TILE_M}, 0:{config.TILE_K}]")
    print(f"   Access pattern: COALESCED (consecutive threads ‚Üí consecutive addresses)")
    
    # Simulate: each thread loads elements
    elements_per_thread = (config.TILE_M * config.TILE_K) // config.threads_per_block
    if thread_id < config.threads_per_block:
        # Thread thread_id loads elements[thread_id * elements_per_thread : ...]
        print(f"   Thread {thread_id} loads {elements_per_thread} elements")
    
    # Actual load (simulation)
    A_shared = A_global[m_start:m_end, :config.TILE_K].clone()
    
    print(f"   ‚úì Loaded {A_shared.numel() * 2} bytes (FP16)\n")
    
    # -------------------------------------------------------------------------
    # STEP 1.2: Load B tile (PACKED INT4)
    # -------------------------------------------------------------------------
    # B is stored in INT4 packed format: 2 weights per byte
    # Must load packed data, will unpack later in registers
    
    print(f"üîÑ Step 1.2: Loading B tile (INT4 packed)")
    print(f"   Source: B_packed_global[{n_start}:{n_end}, 0:{config.TILE_K//2}]")
    print(f"   Dest:   B_shared[0:{config.TILE_N}, 0:{config.TILE_K//2}]")
    print(f"   Format: 2 INT4 values per byte")
    
    # Load packed weights
    B_shared = B_packed_global[n_start:n_end, :config.TILE_K // 2].clone()
    
    print(f"   ‚úì Loaded {B_shared.numel()} bytes (packed INT4)")
    print(f"   Memory saved vs FP16: {(config.TILE_N * config.TILE_K * 2) / B_shared.numel():.1f}√ó\n")
    
    # -------------------------------------------------------------------------
    # STEP 1.3: SYNCHRONIZE
    # -------------------------------------------------------------------------
    # __syncthreads() - wait for all threads to finish loading
    print(f"üîí Step 1.3: __syncthreads()")
    print(f"   All 256 threads reach barrier")
    print(f"   Ensures shared memory is fully populated before compute\n")
    
    return A_shared, B_shared


# ============================================================================
# PHASE 2: INT4 UNPACKING - Shared Memory ‚Üí Registers
# ============================================================================

def phase2_unpack_int4_to_registers(
    B_shared: torch.Tensor,
    thread_id: int,
    warp_id: int,
    lane_id: int
) -> torch.Tensor:
    """
    B∆Ø·ªöC 2: UNPACK INT4 WEIGHTS V√ÄO REGISTERS
    
    Marlin unpacks INT4 on-the-fly trong registers ƒë·ªÉ feed v√†o Tensor Cores.
    M·ªói thread unpacks ph·∫ßn data c·ªßa n√≥.
    
    Thread Organization:
    - warp_id = thread_id // 32 (which warp: 0-7)
    - lane_id = thread_id % 32 (position in warp: 0-31)
    
    Args:
        B_shared: Packed INT4 data trong shared memory
        thread_id: Global thread ID trong block
        warp_id: Warp ID (0-7)
        lane_id: Lane ID trong warp (0-31)
    
    Returns:
        B_unpacked: Unpacked FP16 weights trong registers
    """
    
    print(f"\n{'='*80}")
    print(f"PHASE 2: UNPACKING INT4 TO REGISTERS")
    print(f"Thread {thread_id} | Warp {warp_id} | Lane {lane_id}")
    print(f"{'='*80}\n")
    
    # Each thread processes a subset of weights
    N, K_packed = B_shared.shape  # K_packed = K // 2
    K = K_packed * 2
    
    # Allocate register space (8 FP16 values fit in 4 registers)
    elements_per_thread = 8
    B_unpacked = torch.zeros(elements_per_thread, dtype=torch.float16)
    
    print(f"üì¶ Unpacking INT4 ‚Üí FP16:")
    print(f"   Input: {K_packed} packed bytes per row")
    print(f"   Output: {K} FP16 values per row")
    print(f"   Each thread unpacks {elements_per_thread} values\n")
    
    # -------------------------------------------------------------------------
    # STEP 2.1: FETCH PACKED DATA FROM SHARED MEMORY
    # -------------------------------------------------------------------------
    # Thread fetches its assigned packed bytes
    # Use __shfl_sync() for warp-level communication if needed
    
    print(f"üîÑ Step 2.1: Fetch packed data")
    
    # Each thread reads from different location
    start_idx = (thread_id * elements_per_thread) // 2  # Byte index
    
    if start_idx < K_packed:
        # Read 4 bytes (holds 8 INT4 values)
        packed_bytes = B_shared[0, start_idx:start_idx + elements_per_thread // 2]
        
        print(f"   Thread {thread_id} reads bytes [{start_idx}:{start_idx + 4}]")
        print(f"   Packed values (hex): {[f'{b:02X}' for b in packed_bytes.tolist()]}\n")
        
        # -------------------------------------------------------------------------
        # STEP 2.2: UNPACK EACH BYTE
        # -------------------------------------------------------------------------
        print(f"üîì Step 2.2: Unpack bytes to INT4 values")
        
        for i, byte_val in enumerate(packed_bytes):
            byte_val = byte_val.item()
            
            # Extract two 4-bit values
            val0 = byte_val & 0x0F        # Lower 4 bits
            val1 = (byte_val >> 4) & 0x0F  # Upper 4 bits
            
            # Convert unsigned to signed (4-bit: -8 to 7)
            if val0 >= 8:
                val0 -= 16
            if val1 >= 16:
                val1 -= 16
            
            print(f"   Byte {i} (0x{byte_val:02X}) ‚Üí [{val0:+3d}, {val1:+3d}]")
            
            # Store in registers as INT8 first
            B_unpacked[i * 2] = float(val0)
            B_unpacked[i * 2 + 1] = float(val1)
        
        print()
        
        # -------------------------------------------------------------------------
        # STEP 2.3: SCALE TO FP16
        # -------------------------------------------------------------------------
        # Apply quantization scale (stored separately)
        # scale = max(abs(original_weights)) / 7.0
        
        scale = 0.1  # Example scale factor (would be loaded from memory)
        
        print(f"üìê Step 2.3: Apply scale factor")
        print(f"   Scale: {scale}")
        print(f"   Formula: FP16_value = INT4_value √ó scale")
        
        B_unpacked = B_unpacked * scale
        
        print(f"   Unpacked FP16 values: {B_unpacked.tolist()}\n")
    
    return B_unpacked


# ============================================================================
# PHASE 3: TENSOR CORE / INT CORE COMPUTATION
# ============================================================================

def phase3_tensor_core_matmul(
    A_frag: torch.Tensor,
    B_frag: torch.Tensor,
    warp_id: int,
    lane_id: int
) -> torch.Tensor:
    """
    B∆Ø·ªöC 3: TENSOR CORE COMPUTATION (FP16√óFP16 MMA)
    
    Sau khi unpack INT4‚ÜíFP16, s·ª≠ d·ª•ng Tensor Cores ƒë·ªÉ compute matmul.
    Tensor Cores th·ª±c hi·ªán 16√ó16√ó16 matrix multiply-accumulate (MMA).
    
    Warp-level operation:
    - All 32 threads trong warp collaborate
    - Each thread holds fragment c·ªßa matrix
    - wmma::mma_sync() performs the computation
    
    NOTE: Marlin tr√™n Ampere/Hopper c√≥ th·ªÉ d√πng INT4 Tensor Cores directly,
    nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n ta gi·∫£ s·ª≠ ƒë√£ unpack th√†nh FP16.
    
    Args:
        A_frag: Fragment c·ªßa A matrix (FP16) trong registers
        B_frag: Fragment c·ªßa B matrix (FP16) trong registers
        warp_id: Warp ID
        lane_id: Lane ID trong warp
    
    Returns:
        C_frag: Output fragment (FP16)
    """
    
    print(f"\n{'='*80}")
    print(f"PHASE 3: TENSOR CORE COMPUTATION")
    print(f"Warp {warp_id} | Lane {lane_id}")
    print(f"{'='*80}\n")
    
    # -------------------------------------------------------------------------
    # STEP 3.1: LOAD FRAGMENTS INTO TENSOR CORE REGISTERS
    # -------------------------------------------------------------------------
    print(f"üì• Step 3.1: Load fragments")
    print(f"   A fragment: {A_frag.shape} FP16 values")
    print(f"   B fragment: {B_frag.shape} FP16 values")
    print(f"   All 32 threads in warp hold parts of 16√ó16 matrices\n")
    
    # Each thread holds a piece of the larger matrices
    # For 16√ó16√ó16 MMA:
    # - Each thread in warp holds 8 elements from A, 8 from B
    # - Collectively forms 16√ó16√ó16 operation
    
    # -------------------------------------------------------------------------
    # STEP 3.2: TENSOR CORE MMA INSTRUCTION
    # -------------------------------------------------------------------------
    print(f"‚ö° Step 3.2: Execute Tensor Core MMA")
    print(f"   Instruction: wmma::mma_sync()")
    print(f"   Operation: C[16√ó16] += A[16√ó16] @ B[16√ó16]")
    print(f"   Throughput: ~256 FP16 FLOPS per instruction")
    print(f"   Latency: ~4 cycles\n")
    
    # Simulate MMA (simplified)
    # In reality, this is a single instruction executed by Tensor Core
    if A_frag.numel() == B_frag.numel():
        # Dot product (simplified representation)
        partial_sum = torch.sum(A_frag * B_frag)
        C_frag = torch.tensor([partial_sum] * 8, dtype=torch.float16)
    else:
        C_frag = torch.zeros(8, dtype=torch.float16)
    
    print(f"   Result fragment: {C_frag.shape}")
    print(f"   Partial sum from this thread: {C_frag[0].item():.4f}\n")
    
    # -------------------------------------------------------------------------
    # STEP 3.3: WARP-LEVEL REDUCTION (if needed)
    # -------------------------------------------------------------------------
    print(f"üîÑ Step 3.3: Warp shuffle reduction")
    print(f"   Use __shfl_down_sync() to combine results across lanes")
    print(f"   Lane 0 will hold final sum after log2(32) = 5 steps\n")
    
    # Simulated shuffle reduction
    # In real code: for (int offset = 16; offset > 0; offset /= 2)
    #                   val += __shfl_down_sync(0xffffffff, val, offset);
    
    return C_frag


# ============================================================================
# PHASE 4: ACCUMULATION & WRITE BACK
# ============================================================================

def phase4_accumulate_and_writeback(
    C_accum: torch.Tensor,
    C_frag: torch.Tensor,
    block_id_x: int,
    block_id_y: int,
    thread_id: int,
    C_global: torch.Tensor,
    config: MarlinKernelConfig
):
    """
    B∆Ø·ªöC 4: ACCUMULATE RESULTS V√Ä WRITE BACK TO GLOBAL MEMORY
    
    Accumulate results t·ª´ multiple K iterations.
    Cu·ªëi c√πng write k·∫øt qu·∫£ t·ª´ shared memory ‚Üí global memory.
    
    Args:
        C_accum: Accumulated results trong shared memory
        C_frag: New fragment t·ª´ Tensor Core
        block_id_x, block_id_y: Block indices
        thread_id: Thread ID
        C_global: Output matrix trong global memory
        config: Kernel configuration
    """
    
    print(f"\n{'='*80}")
    print(f"PHASE 4: ACCUMULATION & WRITE BACK")
    print(f"Block [{block_id_x}, {block_id_y}] | Thread {thread_id}")
    print(f"{'='*80}\n")
    
    # -------------------------------------------------------------------------
    # STEP 4.1: ACCUMULATE INTO SHARED MEMORY
    # -------------------------------------------------------------------------
    print(f"‚ûï Step 4.1: Accumulate results")
    print(f"   C_accum += C_frag")
    print(f"   Multiple K iterations accumulate here\n")
    
    # Each thread adds its fragment
    # This happens in registers first, then shared memory
    C_accum += C_frag.sum()
    
    # -------------------------------------------------------------------------
    # STEP 4.2: SYNCHRONIZE BEFORE WRITE
    # -------------------------------------------------------------------------
    print(f"üîí Step 4.2: __syncthreads()")
    print(f"   Ensure all threads finished accumulation\n")
    
    # -------------------------------------------------------------------------
    # STEP 4.3: WRITE TO GLOBAL MEMORY (COALESCED)
    # -------------------------------------------------------------------------
    print(f"üíæ Step 4.3: Write results to global memory")
    
    m_start = block_id_y * config.TILE_M
    m_end = min(m_start + config.TILE_M, config.M)
    n_start = block_id_x * config.TILE_N
    n_end = min(n_start + config.TILE_N, config.N)
    
    print(f"   Dest: C_global[{m_start}:{m_end}, {n_start}:{n_end}]")
    print(f"   Size: {(m_end - m_start) * (n_end - n_start) * 2} bytes (FP16)")
    print(f"   Access: COALESCED write\n")
    
    # Simulate write (each thread writes its portion)
    if thread_id == 0:
        C_global[m_start:m_end, n_start:n_end] = C_accum
        print(f"   ‚úì Block [{block_id_x}, {block_id_y}] completed\n")


# ============================================================================
# PHASE 5: PIPELINE & DOUBLE BUFFERING
# ============================================================================

def phase5_pipeline_execution():
    """
    B∆Ø·ªöC 5: PIPELINING & DOUBLE BUFFERING
    
    Marlin overlaps memory loading v·ªõi computation ƒë·ªÉ hide latency.
    
    Pipeline stages:
    1. Load iteration i+1 weights (async)
    2. Compute iteration i (using previously loaded data)
    3. Write iteration i-1 results
    
    Double buffering:
    - Shared memory split into 2 buffers: A v√† B
    - While computing with buffer A, load next tile into buffer B
    - Swap buffers each iteration
    """
    
    print(f"\n{'='*80}")
    print(f"PHASE 5: PIPELINE & DOUBLE BUFFERING")
    print(f"{'='*80}\n")
    
    print(f"üîÑ Pipeline Timeline (overlapped operations):\n")
    print(f"Iteration 0:")
    print(f"  ‚îú‚îÄ Load tile K[0:128]        [Memory]")
    print(f"  ‚îî‚îÄ (wait)\n")
    
    print(f"Iteration 1:")
    print(f"  ‚îú‚îÄ Load tile K[128:256]      [Memory, Async]")
    print(f"  ‚îî‚îÄ Compute K[0:128]          [Tensor Cores]    ‚Üê OVERLAP!\n")
    
    print(f"Iteration 2:")
    print(f"  ‚îú‚îÄ Load tile K[256:384]      [Memory, Async]")
    print(f"  ‚îú‚îÄ Compute K[128:256]        [Tensor Cores]    ‚Üê OVERLAP!")
    print(f"  ‚îî‚îÄ Write results[0:128]      [Memory]          ‚Üê OVERLAP!\n")
    
    print(f"üíæ Double Buffering:\n")
    print(f"Shared Memory Layout:")
    print(f"  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print(f"  ‚îÇ Buffer A (24 KB)       ‚îÇ ‚Üê Currently computing")
    print(f"  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    print(f"  ‚îÇ Buffer B (24 KB)       ‚îÇ ‚Üê Loading next tile")
    print(f"  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n")
    
    print(f"Benefits:")
    print(f"  ‚Ä¢ Hides memory latency (~200-400 cycles)")
    print(f"  ‚Ä¢ Keeps Tensor Cores busy (>90% utilization)")
    print(f"  ‚Ä¢ Achieves near-peak performance\n")


# ============================================================================
# FULL EXECUTION DEMO
# ============================================================================

def demo_full_marlin_execution():
    """Run complete demonstration of Marlin kernel execution"""
    
    print("\n" + "="*80)
    print(" " * 20 + "MARLIN KERNEL - COMPLETE EXECUTION")
    print("="*80 + "\n")
    
    # Setup
    M, N, K = 128, 512, 512
    config = MarlinKernelConfig(M, N, K)
    config.print_config()
    
    # Create dummy data
    A = torch.randn(M, K, dtype=torch.float16)
    B = torch.randn(N, K, dtype=torch.float16)
    
    # Pack B to INT4
    B_packed = torch.randint(0, 255, (N, K//2), dtype=torch.uint8)
    C = torch.zeros(M, N, dtype=torch.float16)
    
    # Simulate one block execution
    block_x, block_y = 0, 0
    thread_id = 0
    warp_id = thread_id // 32
    lane_id = thread_id % 32
    
    # Phase 1: Load to shared memory
    A_shared, B_shared = phase1_load_to_shared_memory(
        block_x, block_y, thread_id, A, B_packed, config
    )
    
    # Phase 2: Unpack INT4
    B_unpacked = phase2_unpack_int4_to_registers(
        B_shared, thread_id, warp_id, lane_id
    )
    
    # Phase 3: Tensor Core computation
    A_frag = torch.randn(8, dtype=torch.float16)  # Dummy fragment
    C_frag = phase3_tensor_core_matmul(A_frag, B_unpacked, warp_id, lane_id)
    
    # Phase 4: Accumulate and write back
    C_accum = torch.zeros(config.TILE_M, config.TILE_N, dtype=torch.float16)
    phase4_accumulate_and_writeback(
        C_accum, C_frag, block_x, block_y, thread_id, C, config
    )
    
    # Phase 5: Pipeline explanation
    phase5_pipeline_execution()
    
    print("="*80)
    print(" " * 25 + "üéâ EXECUTION COMPLETE!")
    print("="*80 + "\n")
    
    print("üìö Key Takeaways:")
    print("  1. Memory: Global ‚Üí Shared ‚Üí Registers (3-level hierarchy)")
    print("  2. Threads: 256 threads/block organized in 8 warps")
    print("  3. INT4: Unpacked on-the-fly in registers (no FP16 storage)")
    print("  4. Tensor Cores: 16√ó16√ó16 MMA, ~256 FLOPS/instruction")
    print("  5. Pipeline: Overlap load/compute/store for efficiency")
    print("  6. Speedup: 2-4√ó vs FP16 due to memory bandwidth reduction\n")


if __name__ == "__main__":
    demo_full_marlin_execution()
